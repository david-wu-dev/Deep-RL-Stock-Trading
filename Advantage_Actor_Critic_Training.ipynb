{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This jupyter notebook was originally run on Google Colab. If not being run on Google Colab, skip the following drive mounting step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeC4BM2puLbW"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "%cd gdrive/MyDrive/Deep-RL-Stock-Trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jVKp2Ou7n6Ke"
   },
   "outputs": [],
   "source": [
    "from indicators import get_indicators, sma, macd, rsi, cci\n",
    "from environment import StockEnvironment\n",
    "\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the stock data from a csv file that was originally downloaded from kaggle. The data being read stores one set of values for each day the market is open. It contains information about the date, high, low, open, close, and adjusted close for the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "57o2PNktoHii"
   },
   "outputs": [],
   "source": [
    "data_dict = {'Date': [], 'Open': [], 'Close': [], 'High': [], 'Low': [], 'Adj Close': []}\n",
    "\n",
    "with open('./data/stocks/AAPL.csv', newline = '') as csvfile:\n",
    "  reader = csv.DictReader(csvfile)\n",
    "  for row in reader:\n",
    "    data_dict['Date'].append(row['Date'])\n",
    "    data_dict['Open'].append(float(row['Open']))\n",
    "    data_dict['Close'].append(float(row['Close']))\n",
    "    data_dict['High'].append(float(row['High']))\n",
    "    data_dict['Low'].append(float(row['Low']))\n",
    "    data_dict['Adj Close'].append(float(row['Adj Close']))\n",
    "\n",
    "indicators = get_indicators(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor Critic architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "edvvL3aIoVVw"
   },
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Critic, self).__init__()\n",
    "    self.cfc1 = nn.Linear(17, 256)\n",
    "    self.cfc2 = nn.Linear(256, 128)\n",
    "    self.cfc3 = nn.Linear(128, 1)\n",
    "\n",
    "  def forward(self, state, action):\n",
    "    x = torch.cat((state, action), dim=1)\n",
    "    x = F.relu(self.cfc1(x))\n",
    "    x = F.relu(self.cfc2(x))\n",
    "    x = self.cfc3(x)\n",
    "    return x\n",
    "\n",
    "  \n",
    "class Actor(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Actor, self).__init__()\n",
    "    self.afc1 = nn.Linear(8, 256)\n",
    "    self.afc2 = nn.Linear(256, 128)\n",
    "    self.afc3 = nn.Linear(128, 9)\n",
    "  \n",
    "  def forward(self, state):\n",
    "    x = F.relu(self.afc1(state))\n",
    "    x = F.relu(self.afc2(x))\n",
    "    x = self.afc3(x)\n",
    "    return x\n",
    "\n",
    "def normal_init(m):\n",
    "  if isinstance(m, (nn.Linear)):\n",
    "      m.weight.data.normal_(0, .02)\n",
    "      if m.bias.data is not None:\n",
    "          m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper trainer class that allows for easier training. The trainer supports functions that get actions according to an epsilon greedy exploration strategy, optimizing by sampling from the replay memory, adding to the replay memory and clearing the replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1-vE9IEoavH"
   },
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "  def __init__(self):\n",
    "    self.actor = Actor()\n",
    "    self.critic = Critic()\n",
    "\n",
    "    self.actor.apply(normal_init)\n",
    "    self.critic.apply(normal_init)\n",
    "    \n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), .00001)\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), .0001)\n",
    "\n",
    "    self.replay = []\n",
    "    self.eps = .15\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "  \n",
    "  def get_action(self, state):\n",
    "    output = self.actor(state)\n",
    "    probs = self.softmax(output)\n",
    "    \n",
    "    if random.random() < self.eps:\n",
    "      ## explore\n",
    "      index = random.randint(0, 8)\n",
    "      action = torch.zeros((1, 9))\n",
    "      action[0][index] = 1\n",
    "    else:\n",
    "      ## exploit\n",
    "      index = torch.argmax(probs, dim=1).item()\n",
    "      action = torch.zeros((1, 9))\n",
    "      action[0][index] = 1\n",
    "    \n",
    "    return action\n",
    "\n",
    "  \n",
    "  def optimize(self):\n",
    "    ## sample a random (s, a, r, s) tuple from the replay memory\n",
    "    s1, a1, r1, s2 = self.replay[random.randint(0, len(self.replay)-1)]\n",
    "\n",
    "    ## updating the critic\n",
    "    a2 = self.actor(s2).detach()\n",
    "    next_val = self.critic(s2, a2).detach()\n",
    "\n",
    "    expected_val = r1 + .4*next_val\n",
    "    predicted_val = self.critic(s1, a1)\n",
    "\n",
    "    td_error = (expected_val - predicted_val).detach()\n",
    "    critic_loss = torch.square(expected_val - predicted_val)\n",
    "    \n",
    "    self.critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)\n",
    "    self.critic_optimizer.step()\n",
    "\n",
    "    ## updating the actor\n",
    "    output = self.actor(s1)\n",
    "    probs = self.softmax(output)\n",
    "    action_index = torch.argmax(a1, dim=1).item()\n",
    "    log_prob = torch.log(probs[0][action_index])\n",
    "    actor_loss = -td_error * log_prob\n",
    "    \n",
    "    self.actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)\n",
    "    self.actor_optimizer.step()\n",
    "\n",
    "  def clear_replay(self):\n",
    "    self.replay = []\n",
    "\n",
    "  def add_replay(self, x):\n",
    "    self.replay.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tv2cBXFBouNY"
   },
   "outputs": [],
   "source": [
    "env = StockEnvironment(data_dict, indicators, 800, 'AC')\n",
    "state = torch.tensor([env.reset()])\n",
    "trainer = Trainer()\n",
    "terminal = False\n",
    "\n",
    "\n",
    "for epoch in range(50):\n",
    "  action_list = [0] * 9\n",
    "  while not terminal:\n",
    "    action = trainer.get_action(state)\n",
    "    action_index = torch.argmax(action, dim=1).item()\n",
    "    action_list[action_index] += 1\n",
    "    new_state, reward, terminal, info = env.step(action_index)\n",
    "    new_state = torch.tensor([new_state])\n",
    "    trainer.add_replay((state, action, reward, new_state))\n",
    "    state = new_state\n",
    "\n",
    "  for i in range(1000):\n",
    "    trainer.optimize()\n",
    "\n",
    "  trainer.clear_replay()\n",
    "\n",
    "  print('Epoch', epoch)\n",
    "  print(action_list)\n",
    "  env.render()\n",
    "  state = torch.tensor([env.reset()])\n",
    "  terminal = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgSqmcxEsDl_"
   },
   "outputs": [],
   "source": [
    "torch.save(trainer.critic.state_dict(), './trained models/AAPL Critic')\n",
    "torch.save(trainer.actor.state_dict(), './trained models/AAPL Actor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GK-g0JmOtAQk"
   },
   "outputs": [],
   "source": [
    "actor = Actor()\n",
    "actor.load_state_dict(torch.load('./trained models/AAPL Actor'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing some statistics regarding the actions taken during the first 5000 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTVjnLr5peNO"
   },
   "outputs": [],
   "source": [
    "env = StockEnvironment(data_dict, indicators, 5200, 'AC')\n",
    "state = env.reset()\n",
    "counter = 0\n",
    "day = 201\n",
    "action_list = [0] * 9\n",
    "portfolio_value = []\n",
    "terminal = False\n",
    "\n",
    "while not terminal:\n",
    "  action = actor(torch.tensor([state]))\n",
    "  action_index = torch.argmax(action, dim=1).item()\n",
    "  action_list[action_index] += 1\n",
    "  new_state, reward, terminal, info = env.step(action_index)\n",
    "  state = new_state\n",
    "  portfolio_value.append((state[0] + state[1]*data_dict['Close'][day]).item())\n",
    "  counter += 1\n",
    "  day += 1\n",
    "\n",
    "print(counter)\n",
    "print('Actions: ', action_list)\n",
    "env.render()\n",
    "state = env.reset()\n",
    "terminal = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a plot that compares stock price to portfolio value where the starting stock price has be adjusted to be equal to the portfolio value in order to allow for a direct comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twEUEaZUpnW0"
   },
   "outputs": [],
   "source": [
    "other_prices = [num * 367 for num in data_dict['Close']]\n",
    "\n",
    "plt.plot(portfolio_value)\n",
    "plt.plot(other_prices[200:5200])\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('day')\n",
    "plt.legend([\"portfolio\", \"price of stock\"])\n",
    "plt.title('Advantage Actor Critic Trading Agent')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Advantage Actor Critic Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
